Временная сложность алгоритма отражает, насколько быстро он выполняется, и используется для оценки времени работы в зависимости от размера входных данных. Этот показатель также называют вычислительной сложностью.  

Пространственная сложность — это мера объёма памяти, необходимой алгоритму для выполнения. В основном речь идёт об оперативной памяти, хотя в некоторых случаях может использоваться и дисковое пространство.

Оценка скорости алгоритма не должна зависеть от мощности компьютера. Оценка алгоритма производится для худшего случая.

Если использовать пузырьковую сортировку на массиве, элементы которого уже расположены в правильном порядке, алгоритм завершит работу всего за один проход — это и есть наилучший сценарий, при котором время выполнения растёт пропорционально длине массива, то есть линейно.

Однако наихудший случай наступает, когда входной массив отсортирован в обратном порядке. Тогда потребуется максимальное количество перестановок, и время выполнения будет расти квадратично по отношению к размеру массива.

На практике часто встречаются промежуточные ситуации — часть данных может быть упорядочена, часть — нет. В таких случаях производительность пузырьковой сортировки окажется между линейной и квадратичной.

Важно понимать, что оценивать алгоритмы только по лучшему сценарию — нецелесообразно. К примеру, как линейный, так и бинарный поиск теоретически могут найти нужный элемент с первой попытки, но такой результат — скорее исключение. В реальных условиях ключевое значение имеет поведение алгоритма в среднем и худшем случаях.

Таким образом, когда речь заходит о скорости работы алгоритма, чаще всего подразумевается именно худший сценарий — тот, при котором алгоритм выполнит максимально возможное число операций. Такой подход позволяет заранее оценить пределы эффективности и подготовиться к самым ресурсоёмким ситуациям.

Это правило касается не только времени выполнения, но и объёма памяти, которую может потребовать алгоритм — ведь в зависимости от входных данных потребности в памяти тоже могут существенно различаться.

# O-нотация
Чтобы формально описывать такие оценки, особенно в худших сценариях, используют специальную нотацию — так называемую «О большое» (или big O notation на английском). Эта запись помогает выразить, как быстро растут ресурсы, необходимые алгоритму, по мере увеличения объёма входных данных.

Нотация «О большое» служит для того, чтобы отразить, как изменяется продолжительность выполнения алгоритма по мере роста объёма входных данных. Она помогает понять, насколько быстро возрастает время работы алгоритма при увеличении количества обрабатываемых элементов.

Нотация записывается в виде O(), где в скобках указывается характер роста времени выполнения в зависимости от размера входных данных. При этом размер данных традиционно обозначается строчной буквой _n_.

Скорость выполнения алгоритма с использованием нотации O-большое записывается следующим образом:

- **Линейный поиск**: `O(n)` — читается как «О от n». Это означает, что в худшем случае алгоритм выполнит количество операций, пропорциональное числу элементов.
    
- **Бинарный поиск**: `O(log n)` — «О от логарифма n», то есть время выполнения растёт медленно, даже при значительном увеличении объёма данных.
    
- **Сортировка пузырьком**: `O(n²)` — «О от n в квадрате», что указывает на резкий рост числа операций при увеличении количества элементов.

В устной речи часто опускают букву «О» и просто говорят, например, «сложность — логарифм от n», но подразумевается именно нотация O.

Также может использоваться термин **асимптотическая сложность**, который по сути обозначает ту же идею — оценку времени выполнения алгоритма при увеличении входных данных. В рамках данного курса можно считать понятия «асимптотическая сложность» и «О-большое» равнозначными.

Существует множество видов временной сложности алгоритмов, однако в реальной практике чаще всего приходится работать с несколькими ключевыми типами:

- **O(1)** — _постоянное время выполнения_. Алгоритм работает за фиксированное количество шагов, независимо от размера входных данных. Например, получение элемента по индексу в списке или подсчёт длины массива в Python всегда выполняется за одно и то же время, вне зависимости от объёма массива.
    
- **O(log n)** — _логарифмическое время_. Количество операций растёт медленно — по логарифмическому закону. Обычно такие алгоритмы используют метод деления пополам, отбрасывая часть данных на каждом шаге. Типичный пример — бинарный поиск.
    
- **O(n)** — _линейное время_. Алгоритм проходит по всем элементам входных данных, поэтому его выполнение пропорционально их количеству. Пример — линейный поиск, где нужно проверить каждый элемент по очереди.
    
- **O(n log n)** — _линейно-логарифмическое время_. Это типичная сложность для эффективных алгоритмов сортировки, таких как быстрая сортировка или сортировка слиянием. Их производительность выше, чем у квадратичных, но ниже, чем у линейных.
    
- **O(n²)** — _квадратичное время_. Время выполнения быстро возрастает с ростом объёма данных. Алгоритмы с такой сложностью, например пузырьковая сортировка, часто применяются только при небольших массивах.
    
- **O(2ⁿ)** — _экспоненциальное время_. Каждый дополнительный элемент входных данных удваивает число необходимых операций. Такие алгоритмы крайне неэффективны и применимы лишь для небольших задач, например, при полном переборе всех комбинаций.
    
- **O(n!)** — _факториальное время_. Один из самых «тяжёлых» классов сложности. Число операций растёт крайне стремительно. Такие алгоритмы, например, используются для поиска всех возможных перестановок элементов и практически не применимы при работе с реальными большими данными.

Эти оценки позволяют быстро понять, насколько эффективно алгоритм справляется с увеличивающимся объёмом информации.
![[Pasted image 20250528133131.png]]
Ты совершенно правильно описал первую особенность записи сложности через **O-большое**:  
при **асимптотической оценке** (то есть при стремлении размера входных данных к бесконечности) **сохраняют только самый быстро растущий компонент**, а все остальные отбрасывают как **незначительные**.

- `O(n² + n)` → `O(n²)`
    
- `O(n log n + n)` → `O(n log n)`
    
- `O(3n)` → `O(n)` (константы тоже отбрасываются — об этом вторая особенность)

### Вторая особенность: отбрасывание **константных множителей**

Если у алгоритма сложность `O(3n²)` — это всё равно считается как `O(n²)`, потому что при сравнении алгоритмов важна **форма роста**, а не точное количество операций. Константы сильно зависят от реализации, железа и других факторов, а не от самой структуры алгоритма.

Примеры:

- `O(5n)` → `O(n)`
    
- `O(0.5n log n)` → `O(n log n)`
    
- `O(100)` → `O(1)`

**Big O** — это приближённая оценка, фокусирующаяся на **доминирующем факторе роста** при больших объёмах входных данных. Это позволяет сравнивать алгоритмы по эффективности **в долгосрочной перспективе**, не отвлекаясь на несущественные детали.
![[Pasted image 20250528133213.png]]### Вторая особенность: константы можно не учитывать

При оценке сложности алгоритма через нотацию O-большое принято игнорировать постоянные значения, так как они слабо влияют на производительность при больших объёмах данных.

Допустим, алгоритм обрабатывает массив из `n` элементов и для этого выполняет `10n + 5` операций. Несмотря на наличие множителя `10` и слагаемого `5`, сложность алгоритма всё равно записывается как **O(n)**. Почему? Потому что при сравнении с другими алгоритмами важна не точная формула, а то, как быстро растёт количество операций при увеличении `n`.

Рассмотрим пример — напишем функцию, которая считает среднее арифметическое элементов массива. Элементы в массиве возрастают на 1:
```
def calculate_average(data):
    data_sum = 0
    for item in data:
        data_sum += item  # суммируем все элементы
    average = data_sum / len(data)  # делим на длину массива
    return average

print(calculate_average([1, 2, 3, 4, 5]))
```
Оценим, сколько операций выполняет этот код:

- Присваивание нуля переменной `data_sum`: 1 операция.
    
- Суммирование всех `n` элементов: `n` операций.
    
- Получение длины массива: 1 операция.
    
- Деление суммы на длину: ещё 1 операция.

Итого: **n + 3 операций**. В терминах O-большого это **O(n + 3)**, но константу `3` можно опустить, и окончательная оценка сложности будет **O(n)**.

На практике, если массив содержит миллион элементов, общее количество операций составит 1 000 003. Удалив 3, останется 1 000 000 — разница почти незаметна. Поэтому на больших входных данных такие постоянные добавки становятся несущественными. Графики O(n + 3) и O(n) при увеличении `n` выглядят практически одинаково.

### Ускорение за счёт формулы: сложность становится постоянной

Последовательность, в которой каждый следующий элемент отличается от предыдущего на одну и ту же величину, называется **арифметической прогрессией**.

Для таких последовательностей можно не суммировать элементы по одному, а использовать готовую формулу:  
**(первый элемент + последний элемент) × количество элементов ÷ 2**

Заменим цикл в предыдущем алгоритме на эту формулу:
```
def calculate_average(data):
    len_data = len(data)  # сохраняем длину, чтобы не считать дважды
    data_sum = (data[0] + data[-1]) * len_data / 2  # сумма прогрессии
    average = data_sum / len_data  # находим среднее
    return average

print(calculate_average([1, 2, 3, 4, 5]))
```
Теперь количество операций не зависит от длины массива. Неважно, сколько в нём элементов — 5, 500 или 5 миллионов — код выполнит одни и те же действия:

1. Получение длины массива.
    
2. Доступ к первому элементу.
    
3. Доступ к последнему элементу.
    
4. Сложение двух чисел.
    
5. Умножение результата на длину.
    
6. Деление на 2.
    
7. Деление на длину массива.

Всего **7 операций** — и это фиксированное число, которое не меняется с ростом входных данных. Можно записать сложность как **O(7)** или даже **O(7 × 1)**, но в нотации O-большое **константы отбрасываются**, потому что они не влияют на масштаб.

Окончательная сложность — **O(1)**, то есть **постоянная**: сколько бы элементов ни было в массиве, время выполнения остаётся одинаковым.

Если построить график и сравнить этот алгоритм с предыдущим (где использовался цикл и была сложность **O(n)**), видно, что **линия для O(1)** идёт строго горизонтально, в то время как линия **O(n)** растёт вместе с размером данных. Даже **O(7)** и **O(1)** будут выглядеть как одна линия — потому что в масштабах больших входов разница между 1 и 7 несущественна.

### Почему на графиках O(1) и O(7) почти неотличимы

Если изобразить на одном графике функции **O(n)**, **O(1)** и **O(7)**, можно заметить интересную вещь: кривые для **O(1)** и **O(7)** практически совпадают и сливаются в одну горизонтальную линию. Это происходит потому, что при сравнении алгоритмов **разного порядка сложности** (например, линейного и постоянного) **константы теряют значение** — они не влияют на скорость роста.

---

### Как ведут себя функции на маленьких и больших значениях

На **небольших значениях n** (размеров входных данных) ситуация меняется. Там **константы и коэффициенты** могут ощутимо влиять на результат.

Например:

- Если сравнить **линейную функцию** (оранжевая линия) с **квадратичной** (сиреневая), то при **n от 1 до 8** линейный алгоритм работает **медленнее** — ведь у него большой коэффициент перед n.
    
- Точно так же, при **очень малых входах** (например, когда массив состоит из 1–2 элементов), **линейный алгоритм может обогнать постоянный** (зелёная линия), особенно если у постоянного алгоритма большое количество фиксированных операций (например, те же 7).
    

---

### Сложная функция: смесь разных сложностей

На графике также можно увидеть **жёлтую линию** — она показывает рост количества операций для алгоритма, который объединяет в себе сразу несколько компонентов: **квадратичный**, **линейный** и **константный**. В этом случае, на **небольших значениях n** такая функция будет расти **быстрее**, чем обычная квадратичная. Это ещё раз подчёркивает: **на малых входах поведение функций может сильно отличаться от поведения на больших**, но при **асимптотическом анализе** (на больших n) главную роль играет **тип зависимости**, а не константы и коэффициенты.

### Как ведут себя графики при больших значениях n

При больших значениях **n** графики функций группируются в две основные категории:

- **Константные** (зелёная линия) и **линейные** (оранжевая линия) функции хотя и сильно отличаются по времени выполнения между собой, но по сравнению с квадратичными функциями (жёлтая и сиреневая линии) они выглядят почти одинаково — их графики «приклеиваются» к оси координат и кажутся очень маленькими.
    
- Графики квадратичных функций (сиреневая и жёлтая линии) тоже ведут себя очень похоже, даже несмотря на то, что жёлтый график включает дополнительные линейные и константные слагаемые.
### Почему важен анализ на больших данных

Именно поэтому **оценка эффективности алгоритмов** должна проводиться на **больших объёмах входных данных**. Только тогда разница между классами алгоритмов становится заметной и однозначной.
### А что насчёт экспоненциальной сложности?

Если на том же графике изобразить функцию с **экспоненциальной сложностью O(2ⁿ)**, то все квадратичные функции — будь то **n²**, **2n²** или даже **1000n²** — будут выглядеть практически одинаково и «приклеятся» к оси X.

Пример для наглядности:

- Для массива из 1000 элементов квадратичный алгоритм выполнит примерно 1 000 000 операций.
    
- А вот экспоненциальный алгоритм выполнит около 210002^{1000}21000 операций — это число с 302 знаками: